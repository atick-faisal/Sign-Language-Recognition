{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 22:34:18.630431: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import config\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import (\n",
    "    create_img_grid,\n",
    "    create_img_stack,\n",
    "    extract_flxion_features,\n",
    "    SpatialProjection\n",
    ")\n",
    "\n",
    "from model import ProjectionNet\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/dataset/raw/\"\n",
    "subjects = os.listdir(data_dir)\n",
    "\n",
    "augmentation_levels = [0, 9, 7, 11, 13]\n",
    "sp_augment = [\n",
    "    SpatialProjection(\n",
    "    img_dir=\"../../data/dataset/images/\",\n",
    "    # img_len=math.floor(config.IMG_LEN / 3),\n",
    "    img_len=config.IMG_LEN,\n",
    "    polyfit_degree=degree\n",
    ")\n",
    "    for degree in augmentation_levels ]\n",
    "\n",
    "test_subject = \"007\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [08:36<00:00, 73.73s/it]\n"
     ]
    }
   ],
   "source": [
    "train_features = []\n",
    "train_images = []\n",
    "train_labels = []\n",
    "test_features = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for subject in tqdm(subjects):\n",
    "    for gesture in config.GESTURES:\n",
    "        gesture_dir = os.path.join(data_dir, subject, gesture)\n",
    "        recordings = os.listdir(gesture_dir)\n",
    "        for recording in recordings:\n",
    "            file_path = os.path.join(gesture_dir, recording)\n",
    "\n",
    "            data = pd.read_csv(file_path)\n",
    "            data.drop(columns=[\"time\"], inplace=True)\n",
    "            data.drop(0, inplace=True)  # Remove first All-0 row\n",
    "\n",
    "            if data.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            # Flag for determining Trainning and Testing Samples\n",
    "            for_training = random.randint(1, 100) <= 80\n",
    "\n",
    "            for sp in sp_augment:\n",
    "                _images = []\n",
    "                for landmark in config.PROJECTION_LANDMARKS:\n",
    "                    _images.extend(\n",
    "                        sp.get_projection_images(\n",
    "                            data=data.filter(regex=landmark),\n",
    "                            subject=subject,\n",
    "                            gesture=gesture\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                _features = extract_flxion_features(data)\n",
    "\n",
    "                # img = create_img_grid(_images, config.IMG_LEN)\n",
    "                img = create_img_stack(_images[:3])\n",
    "\n",
    "                if for_training:\n",
    "                    train_features.append(_features)\n",
    "                    train_images.append(img)\n",
    "                    train_labels.append(config.GESTURES.index(gesture))\n",
    "                else:\n",
    "                    test_features.append(_features)\n",
    "                    test_images.append(img)\n",
    "                    test_labels.append(config.GESTURES.index(gesture))\n",
    "                    break\n",
    "\n",
    "    #             plt.imshow(img)\n",
    "    #             # plt.savefig(\"../assets/projection_demo.pdf\")\n",
    "    #             plt.show()\n",
    "\n",
    "    #         break\n",
    "    #     break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack CNN Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4655, 150, 1)\n",
      "(4655, 160, 160)\n",
      "(228, 150, 1)\n",
      "(228, 160, 160)\n",
      "(4655,)\n",
      "(228,)\n"
     ]
    }
   ],
   "source": [
    "train_features = np.array(train_features)\n",
    "train_images = np.array(train_images, dtype=\"uint8\")\n",
    "test_features = np.array(test_features)\n",
    "test_images = np.array(test_images, dtype=\"uint8\")\n",
    "\n",
    "X_train = np.split(train_features, 5, axis=-1) + \\\n",
    "    [np.squeeze(img) for img in np.split(train_images, 3, axis=-1)]\n",
    "\n",
    "X_test = np.split(test_features, 5, axis=-1) + \\\n",
    "    [np.squeeze(img) for img in np.split(test_images, 3, axis=-1)]\n",
    "\n",
    "y_train = np.array(train_labels, dtype=\"uint8\")\n",
    "y_test = np.array(test_labels, dtype=\"uint8\")\n",
    "\n",
    "print(X_train[0].shape)\n",
    "print(X_train[-1].shape)\n",
    "print(X_test[0].shape)\n",
    "print(X_test[-1].shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array(images, dtype=\"uint8\")\n",
    "# X = np.expand_dims(X[:, :, :, 0], axis=-1)\n",
    "# # X = np.repeat(np.expand_dims(X, axis=-1), 3, axis=-1)\n",
    "# y = np.array(labels, dtype=\"uint8\")\n",
    "\n",
    "# X_train = np.array(train_images, dtype=\"uint8\")\n",
    "# X_test = np.array(test_images, dtype=\"uint8\")\n",
    "\n",
    "# y_train = np.array(train_labels, dtype=\"uint8\")\n",
    "# y_test = np.array(test_labels, dtype=\"uint8\")\n",
    "\n",
    "# X_train = np.expand_dims(X_train[:, :, :, 0], axis=-1)\n",
    "# X_test = np.expand_dims(X_test[:, :, :, 0], axis=-1)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/dataset/processed/007/y_test_stack_11.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = f\"../../data/dataset/processed/{test_subject}/\"\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "joblib.dump(X_train, os.path.join(save_dir, \"X_train_stack_11.joblib\"))\n",
    "joblib.dump(y_train, os.path.join(save_dir, \"y_train_stcak_11.joblib\"))\n",
    "joblib.dump(X_test, os.path.join(save_dir, \"X_test_stack_11.joblib\"))\n",
    "joblib.dump(y_test, os.path.join(save_dir, \"y_test_stack_11.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.33, random_state=42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "# base_model = tf.keras.applications.MobileNetV2(\n",
    "#     input_shape=(config.IMG_LEN, config.IMG_LEN, 3),\n",
    "#     include_top=False,\n",
    "#     weights=\"imagenet\"\n",
    "# )\n",
    "# base_model.trainable = True\n",
    "# global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "# prediction_layer = tf.keras.layers.Dense(len(config.GESTURES))\n",
    "\n",
    "# inputs = tf.keras.Input(shape=(config.IMG_LEN, config.IMG_LEN, 3))\n",
    "# x = preprocess_input(inputs)\n",
    "# x = base_model(x, training=True)\n",
    "# x = global_average_layer(x)\n",
    "# x = tf.keras.layers.Dropout(0.6)(x)\n",
    "# outputs = prediction_layer(x)\n",
    "# model = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Rescaling(\n",
    "#         1/255.0,\n",
    "#         input_shape=(config.IMG_LEN, config.IMG_LEN, config.N_CHANNELS)\n",
    "#     ),\n",
    "#     tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2, 2)),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     tf.keras.layers.Dense(len(config.GESTURES))\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import config\n",
    "# from model import ConvMixer\n",
    "\n",
    "# model = ConvMixer(\n",
    "#     img_size=config.IMG_LEN,\n",
    "#     in_channels=1,\n",
    "#     n_classes=len(config.GESTURES),\n",
    "#     n_filters=64,\n",
    "#     depth=3,\n",
    "#     kernel_size=5,\n",
    "#     patch_size=2\n",
    "# ).model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 22:42:58.560164: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-22 22:42:58.560235: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Andromeda\n",
      "2022-06-22 22:42:58.560245: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Andromeda\n",
      "2022-06-22 22:42:58.560445: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.48.7\n",
      "2022-06-22 22:42:58.560484: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.48.7\n",
      "2022-06-22 22:42:58.560492: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.48.7\n",
      "2022-06-22 22:42:58.561393: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(config.IMG_LEN, config.IMG_LEN, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "model = ProjectionNet(\n",
    "    img_size=config.IMG_LEN,\n",
    "    segment_len=150,\n",
    "    n_classes=len(config.GESTURES),\n",
    "    base_model=base_model\n",
    ").get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='input_7'), name='input_7', description=\"created by layer 'input_7'\"), but it was called on an input with incompatible shape (None, 160, 160).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='input_8'), name='input_8', description=\"created by layer 'input_8'\"), but it was called on an input with incompatible shape (None, 160, 160).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='input_9'), name='input_9', description=\"created by layer 'input_9'\"), but it was called on an input with incompatible shape (None, 160, 160).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 160, 160).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"mobilenetv2_1.00_160\" (type Functional).\n    \n    Input 0 of layer \"Conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, 160, 160)\n    \n    Call arguments received by layer \"mobilenetv2_1.00_160\" (type Functional):\n      • inputs=tf.Tensor(shape=(None, 160, 160), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=0'>1</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=1'>2</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=2'>3</a>\u001b[0m         monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=5'>6</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=6'>7</a>\u001b[0m ]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=8'>9</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=9'>10</a>\u001b[0m     x\u001b[39m=\u001b[39;49mX_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=10'>11</a>\u001b[0m     y\u001b[39m=\u001b[39;49my_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=11'>12</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_test, y_test),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=12'>13</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=13'>14</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m700\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=14'>15</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=15'>16</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ai/Web/Sign-Language-Recognition/core/src/ProjectionNet.ipynb#ch0000013?line=16'>17</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filegoflzvh0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/ai/Web/Sign-Language-Recognition/core/.env/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"mobilenetv2_1.00_160\" (type Functional).\n    \n    Input 0 of layer \"Conv1\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, 160, 160)\n    \n    Call arguments received by layer \"mobilenetv2_1.00_160\" (type Functional):\n      • inputs=tf.Tensor(shape=(None, 160, 160), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=30,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=700,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    # \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    # \"font.serif\": [\"Computer Modern Roman\"],\n",
    "    \"font.size\": 22,\n",
    "    \"text.color\": \"#212121\",\n",
    "    \"axes.edgecolor\": \"#212121\",\n",
    "    \"xtick.color\": \"#212121\",\n",
    "    \"ytick.color\": \"#212121\",\n",
    "    \"axes.labelcolor\": \"#212121\",\n",
    "    'legend.frameon': False,\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.gca()\n",
    "ax.plot(history.history[\"loss\"], \"-\", color=\"#212121\", label=\"Train Loss\")\n",
    "ax.plot(history.history[\"val_loss\"], \"--\",\n",
    "        color=\"#212121\", label=\"Validation Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/lc.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f0f52ed645d8567bae68a8c372449e0a23f49f10e778396b1f58fd2946c160c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

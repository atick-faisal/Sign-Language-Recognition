{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-26 03:48:45.846736: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import joblib\n",
    "import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import (\n",
    "    create_img_grid,\n",
    "    create_img_stack,\n",
    "    extract_flxion_features,\n",
    "    SpatialProjection\n",
    ")\n",
    "\n",
    "\n",
    "from model.projectionnet.tf import ProjectionNet\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\" \n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../data/dataset/raw/\"\n",
    "subjects = os.listdir(data_dir)\n",
    "\n",
    "augmentation_levels = [0, 9, 7, 11, 13]\n",
    "sp_augment = [\n",
    "    SpatialProjection(\n",
    "    img_dir=\"../../../data/dataset/images/\",\n",
    "    # img_len=math.floor(config.IMG_LEN / 3),\n",
    "    img_len=config.IMG_LEN,\n",
    "    polyfit_degree=degree\n",
    ")\n",
    "    for degree in augmentation_levels ]\n",
    "\n",
    "test_subject = \"007\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = []\n",
    "# train_images = []\n",
    "# train_labels = []\n",
    "# test_features = []\n",
    "# test_images = []\n",
    "# test_labels = []\n",
    "\n",
    "# for subject in tqdm(subjects):\n",
    "#     for gesture in config.GESTURES:\n",
    "#         gesture_dir = os.path.join(data_dir, subject, gesture)\n",
    "#         recordings = os.listdir(gesture_dir)\n",
    "#         for recording in recordings:\n",
    "#             file_path = os.path.join(gesture_dir, recording)\n",
    "\n",
    "#             data = pd.read_csv(file_path)\n",
    "#             data.drop(columns=[\"time\"], inplace=True)\n",
    "#             data.drop(0, inplace=True)  # Remove first All-0 row\n",
    "\n",
    "#             if data.shape[0] == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # ... Flag for determining Trainning and Testing Samples\n",
    "#             for_training = random.randint(1, 100) <= 80\n",
    "\n",
    "#             for sp in sp_augment:\n",
    "#                 _images = []\n",
    "#                 for landmark in config.PROJECTION_LANDMARKS:\n",
    "#                     _images.extend(\n",
    "#                         sp.get_projection_images(\n",
    "#                             data=data.filter(regex=landmark),\n",
    "#                             subject=subject,\n",
    "#                             gesture=gesture\n",
    "#                         )\n",
    "#                     )\n",
    "\n",
    "#                 _features = extract_flxion_features(data)\n",
    "\n",
    "#                 # img = create_img_grid(_images, config.IMG_LEN)\n",
    "#                 img = create_img_stack(_images[:3])\n",
    "\n",
    "#                 if for_training:\n",
    "#                     train_features.append(_features)\n",
    "#                     train_images.append(img)\n",
    "#                     train_labels.append(config.GESTURES.index(gesture))\n",
    "#                 else:\n",
    "#                     test_features.append(_features)\n",
    "#                     test_images.append(img)\n",
    "#                     test_labels.append(config.GESTURES.index(gesture))\n",
    "#                     break\n",
    "\n",
    "#     #             plt.imshow(img)\n",
    "#     #             # plt.savefig(\"../assets/projection_demo.pdf\")\n",
    "#     #             plt.show()\n",
    "\n",
    "#     #         break\n",
    "#     #     break\n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = np.array(train_features)\n",
    "# train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack CNN Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = np.array(train_features)\n",
    "# train_images = np.array(train_images, dtype=\"uint8\")\n",
    "# test_features = np.array(test_features)\n",
    "# test_images = np.array(test_images, dtype=\"uint8\")\n",
    "\n",
    "# X_train = np.split(train_features, len(config.DIST_FEATURES), axis=-1) + \\\n",
    "#     [np.squeeze(img) for img in np.split(train_images, 3, axis=-1)]\n",
    "\n",
    "# X_test = np.split(test_features, len(config.DIST_FEATURES), axis=-1) + \\\n",
    "#     [np.squeeze(img) for img in np.split(test_images, 3, axis=-1)]\n",
    "\n",
    "# y_train = np.array(train_labels, dtype=\"uint8\")\n",
    "# y_test = np.array(test_labels, dtype=\"uint8\")\n",
    "\n",
    "# print(X_train[0].shape)\n",
    "# print(X_train[-1].shape)\n",
    "# print(X_test[0].shape)\n",
    "# print(X_test[-1].shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array(images, dtype=\"uint8\")\n",
    "# X = np.expand_dims(X[:, :, :, 0], axis=-1)\n",
    "# # X = np.repeat(np.expand_dims(X, axis=-1), 3, axis=-1)\n",
    "# y = np.array(labels, dtype=\"uint8\")\n",
    "\n",
    "# X_train = np.array(train_images, dtype=\"uint8\")\n",
    "# X_test = np.array(test_images, dtype=\"uint8\")\n",
    "\n",
    "# y_train = np.array(train_labels, dtype=\"uint8\")\n",
    "# y_test = np.array(test_labels, dtype=\"uint8\")\n",
    "\n",
    "# X_train = np.expand_dims(X_train[:, :, :, 0], axis=-1)\n",
    "# X_test = np.expand_dims(X_test[:, :, :, 0], axis=-1)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f\"../../data/dataset/processed/{test_subject}/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "# joblib.dump(X_train, os.path.join(save_dir, \"X_train_stack_11.joblib\"))\n",
    "# joblib.dump(y_train, os.path.join(save_dir, \"y_train_stcak_11.joblib\"))\n",
    "# joblib.dump(X_test, os.path.join(save_dir, \"X_test_stack_11.joblib\"))\n",
    "# joblib.dump(y_test, os.path.join(save_dir, \"y_test_stack_11.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = joblib.load(os.path.join(save_dir, \"X_train_stack_11.joblib\"))\n",
    "y_train = joblib.load(os.path.join(save_dir, \"y_train_stcak_11.joblib\"))\n",
    "X_test = joblib.load(os.path.join(save_dir, \"X_test_stack_11.joblib\"))\n",
    "y_test = joblib.load(os.path.join(save_dir, \"y_test_stack_11.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.33, random_state=42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "# base_model = tf.keras.applications.MobileNetV2(\n",
    "#     input_shape=(config.IMG_LEN, config.IMG_LEN, 3),\n",
    "#     include_top=False,\n",
    "#     weights=\"imagenet\"\n",
    "# )\n",
    "# base_model.trainable = True\n",
    "# global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "# prediction_layer = tf.keras.layers.Dense(len(config.GESTURES))\n",
    "\n",
    "# inputs = tf.keras.Input(shape=(config.IMG_LEN, config.IMG_LEN, 3))\n",
    "# x = preprocess_input(inputs)\n",
    "# x = base_model(x, training=True)\n",
    "# x = global_average_layer(x)\n",
    "# x = tf.keras.layers.Dropout(0.6)(x)\n",
    "# outputs = prediction_layer(x)\n",
    "# model = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Rescaling(\n",
    "#         1/255.0,\n",
    "#         input_shape=(config.IMG_LEN, config.IMG_LEN, config.N_CHANNELS)\n",
    "#     ),\n",
    "#     tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "#     tf.keras.layers.MaxPool2D((2, 2)),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     tf.keras.layers.Dense(len(config.GESTURES))\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import config\n",
    "# from model import ConvMixer\n",
    "\n",
    "# model = ConvMixer(\n",
    "#     img_size=config.IMG_LEN,\n",
    "#     in_channels=1,\n",
    "#     n_classes=len(config.GESTURES),\n",
    "#     n_filters=64,\n",
    "#     depth=3,\n",
    "#     kernel_size=5,\n",
    "#     patch_size=2\n",
    "# ).model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(config.IMG_LEN, config.IMG_LEN, config.N_CHANNELS),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "model = ProjectionNet(\n",
    "    img_size=config.IMG_LEN,\n",
    "    segment_len=config.SEGMENT_LEN,\n",
    "    n_classes=len(config.GESTURES),\n",
    "    base_model=base_model\n",
    ").get_model(\n",
    "    n_projections=config.N_CHANNELS,\n",
    "    n_channels=len(config.DIST_FEATURES)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=30,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=8,\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    # \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    # \"font.serif\": [\"Computer Modern Roman\"],\n",
    "    \"font.size\": 22,\n",
    "    \"text.color\": \"#212121\",\n",
    "    \"axes.edgecolor\": \"#212121\",\n",
    "    \"xtick.color\": \"#212121\",\n",
    "    \"ytick.color\": \"#212121\",\n",
    "    \"axes.labelcolor\": \"#212121\",\n",
    "    'legend.frameon': False,\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.gca()\n",
    "ax.plot(history.history[\"loss\"], \"-\", color=\"#212121\", label=\"Train Loss\")\n",
    "ax.plot(history.history[\"val_loss\"], \"--\",\n",
    "        color=\"#212121\", label=\"Validation Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/lc.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f0f52ed645d8567bae68a8c372449e0a23f49f10e778396b1f58fd2946c160c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
